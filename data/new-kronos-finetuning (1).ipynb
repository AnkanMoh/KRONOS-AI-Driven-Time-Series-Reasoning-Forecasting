{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, warnings, numpy as np\nprint(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\nwarnings.filterwarnings(\"ignore\"); np.seterr(all=\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:31:19.547068Z","iopub.execute_input":"2025-09-11T13:31:19.547335Z","iopub.status.idle":"2025-09-11T13:31:22.698630Z","shell.execute_reply.started":"2025-09-11T13:31:19.547313Z","shell.execute_reply":"2025-09-11T13:31:22.697754Z"}},"outputs":[{"name":"stdout","text":"Torch: 2.6.0+cu124 | CUDA: True\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"%%writefile config.yaml\ntrain_start_date: \"2018-01-01\"\ntrain_end_date:   \"2025-05-30\"\nvalid_start_date: \"2025-06-01\"\nvalid_end_date:   \"2025-08-22\"\n\nticker_list:\n  - NVDA\n  - AMD\n  - INTC\n  - TXN\n  - MCHP\n  - QCOM\n  - MSFT\n  - ORCL\n  - NOW\n  - CSCO\n  - IBM\n  - INTU\n  - SMCI\n  - AAPL\n  - AMZN\n  - TSLA\n  - GOOGL\n  - META\n  - PYPL\n  - GPN\n  - CWEN\n  - FSLR\n  - SEDG\n  - BE\n  - MNTK\n  - AMTX\n  - CLNE\n  - CEG\n  - BN\n  - NEE\n  - COP\n  - DVN\n  - ENB\n  - XOM\n  - PSX\n  - SHEL\n  - MPC\n  - ED\n  - PPL\n  - SO\n\ncontext_length: 160\nbatch_size: 128\ngrad_accum_steps: 2        \nlearning_rate: 0.0003\nweight_decay: 0.0001\nepochs: 18\nd_model: 160\nn_heads: 5\nn_layers: 3\ndropout: 0.15\nrandom_seed: 123\ndevice: \"auto\"\n\nlambda_dir: 0.25\nuse_indicators: true\nper_ticker_scaler: true\ntargets: [\"Open\",\"Close\",\"Low\",\"High\",\"Volume\"]\n\nartifacts_dir: \"artifacts\"\ncache_dir: \"artifacts/cache\"\nmodel_path: \"artifacts/kronos_p100.pth\"\nscaler_path: \"artifacts/scalers.pkl\"\npredictions_path: \"artifacts/predictions.json\"\nevaluation_csv: \"artifacts/evaluation.csv\"\n\nuse_kronos_base: false\nkronos_repo: \"NeoQuasar/Kronos-base\"\nkronos_tokenizer_repo: \"NeoQuasar/Kronos-Tokenizer-base\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:31:40.393090Z","iopub.execute_input":"2025-09-11T13:31:40.394048Z","iopub.status.idle":"2025-09-11T13:31:40.399150Z","shell.execute_reply.started":"2025-09-11T13:31:40.394016Z","shell.execute_reply":"2025-09-11T13:31:40.398533Z"}},"outputs":[{"name":"stdout","text":"Writing config.yaml\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile util.py\nimport os, pickle, yaml\nfrom dataclasses import dataclass\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\n\nTARGETS = [\"Open\",\"Close\",\"Low\",\"High\",\"Volume\"]\n\ndef load_cfg(path: str) -> dict:\n    with open(path, \"r\") as f: return yaml.safe_load(f)\n\ndef ensure_dir(path: str): os.makedirs(path, exist_ok=True)\n\ndef set_seed(seed: int = 123):\n    import random\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\ndef yf_download(ticker: str, start: str, end: str) -> pd.DataFrame:\n    end_exc = (pd.to_datetime(end) + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    df = yf.download(ticker, start=start, end=end_exc, progress=False, auto_adjust=False, group_by=\"column\")\n    if df is None or df.empty: return pd.DataFrame()\n    if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)\n    keep = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n    for k in keep:\n        if k not in df.columns: df[k] = np.nan\n    df = df[keep].dropna()\n    df.index = pd.to_datetime(df.index).tz_localize(None)\n    df = df[~df.index.duplicated(keep=\"last\")]\n    return df\n\ndef add_indicators(df: pd.DataFrame) -> pd.DataFrame:\n    from ta.trend import SMAIndicator, MACD\n    from ta.momentum import RSIIndicator\n    from ta.volatility import BollingerBands\n    out = df.copy(); close = out[\"Close\"].astype(float)\n    for w in (5, 10, 20, 50): out[f\"SMA{w}\"] = SMAIndicator(close, window=w).sma_indicator()\n    out[\"RSI14\"] = RSIIndicator(close, window=14).rsi()\n    macd = MACD(close)\n    out[\"MACD\"] = macd.macd(); out[\"MACD_Signal\"] = macd.macd_signal(); out[\"MACD_Hist\"] = macd.macd_diff()\n    bb = BollingerBands(close, window=20, window_dev=2)\n    out[\"BB_High\"] = bb.bollinger_hband(); out[\"BB_Low\"]  = bb.bollinger_lband()\n    out[\"Ret1\"] = close.pct_change(); out[\"LogRet1\"] = np.log(close.replace(0, np.nan)).diff(); out[\"Vol20\"] = out[\"LogRet1\"].rolling(20).std()\n    out = out.replace([np.inf, -np.inf], np.nan).dropna()\n    return out\n\ndef build_features(df: pd.DataFrame, use_indicators: bool) -> pd.DataFrame:\n    return add_indicators(df) if use_indicators else df.copy()\n\ndef load_or_build_features(ticker: str, cfg: dict, full_end: str) -> pd.DataFrame:\n    ensure_dir(cfg[\"cache_dir\"])\n    cache_path = os.path.join(cfg[\"cache_dir\"], f\"{ticker}.parquet\")\n    if os.path.exists(cache_path):\n        try:\n            df = pd.read_parquet(cache_path)\n            # quick freshness check\n            if df.index.max().date() >= pd.to_datetime(cfg[\"valid_end_date\"]).date():\n                return df\n        except Exception:\n            pass\n    raw = yf_download(ticker, cfg[\"train_start_date\"], full_end)\n    if raw.empty: return raw\n    feat = build_features(raw, cfg.get(\"use_indicators\", True))\n    feat.to_parquet(cache_path)\n    return feat\n\n@dataclass\nclass TickerScalers:\n    x: StandardScaler\n    y_price: StandardScaler\n\ndef fit_scalers_per_ticker(train_df: pd.DataFrame, feature_cols: List[str]) -> TickerScalers:\n    xs = train_df[feature_cols].values.astype(np.float32)\n    prices = train_df[[\"Open\",\"Close\",\"Low\",\"High\"]].values.astype(np.float32)\n    sx = StandardScaler().fit(xs); sy_price = StandardScaler().fit(prices)\n    return TickerScalers(sx, sy_price)\n\nclass WindowDataset(Dataset):\n    def __init__(self, x, prices, volumes, ctx, sx: StandardScaler, sy_price: StandardScaler, ticker_id: int):\n        self.ctx = int(ctx)\n        self.X = (x - sx.mean_) / np.where(sx.scale_==0, 1.0, sx.scale_)\n        self.P = (prices - sy_price.mean_) / np.where(sy_price.scale_==0, 1.0, sy_price.scale_)\n        self.V = np.log1p(volumes)\n        self.T = x.shape[0]\n        self.ticker_id = ticker_id\n    def __len__(self): return max(0, self.T - self.ctx - 1)\n    def __getitem__(self, idx):\n        t = idx + self.ctx - 1\n        x_ctx = self.X[t-(self.ctx-1):t+1]\n        y_price_next = self.P[t+1]\n        y_vol_next = self.V[t+1]\n        return (torch.from_numpy(x_ctx).float(),\n                torch.from_numpy(y_price_next).float(),\n                torch.tensor(y_vol_next, dtype=torch.float32))\n\nclass Encoder(nn.Module):\n    def __init__(self, f_in: int, d_model=160, n_heads=5, n_layers=3, dropout=0.15):\n        super().__init__()\n        self.proj = nn.Linear(f_in, d_model)\n        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model, dropout=dropout, batch_first=True, norm_first=True)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, x):\n        z = self.proj(x)\n        h = self.encoder(z)\n        return self.norm(h[:, -1, :])\n\nclass Heads(nn.Module):\n    def __init__(self, d_model=160, hidden=160, dropout=0.15):\n        super().__init__()\n        self.shared = nn.Sequential(nn.Linear(d_model, hidden), nn.GELU(), nn.Dropout(dropout))\n        self.price = nn.Linear(hidden, 4)\n        self.vol   = nn.Linear(hidden, 1)\n    def forward(self, h):\n        s = self.shared(h)\n        return self.price(s), self.vol(s)\n\nclass KronosP100(nn.Module):\n    def __init__(self, f_in, cfg):\n        super().__init__()\n        self.enc = Encoder(f_in, cfg.get(\"d_model\",160), cfg.get(\"n_heads\",5), cfg.get(\"n_layers\",3), cfg.get(\"dropout\",0.15))\n        self.heads = Heads(cfg.get(\"d_model\",160), hidden=cfg.get(\"d_model\",160), dropout=cfg.get(\"dropout\",0.15))\n    def forward(self, x):\n        h = self.enc(x)\n        return self.heads(h)\n\ndef device_from_cfg(cfg):\n    if cfg.get(\"device\") == \"cpu\": return torch.device(\"cpu\")\n    if cfg.get(\"device\") == \"cuda\" and torch.cuda.is_available(): return torch.device(\"cuda\")\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:31:46.067592Z","iopub.execute_input":"2025-09-11T13:31:46.068232Z","iopub.status.idle":"2025-09-11T13:31:46.074650Z","shell.execute_reply.started":"2025-09-11T13:31:46.068206Z","shell.execute_reply":"2025-09-11T13:31:46.073970Z"}},"outputs":[{"name":"stdout","text":"Writing util.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile train.py\nimport os, pickle\nimport numpy as np, pandas as pd\nimport torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\nfrom util import (load_cfg, ensure_dir, set_seed, build_features, load_or_build_features,\n                  WindowDataset, fit_scalers_per_ticker, KronosP100, device_from_cfg)\n\ndef main():\n    cfg = load_cfg(\"config.yaml\")\n    set_seed(cfg.get(\"random_seed\",123))\n    ensure_dir(cfg[\"artifacts_dir\"]); ensure_dir(cfg[\"cache_dir\"])\n    dev = device_from_cfg(cfg)\n\n    ctx = int(cfg[\"context_length\"])\n    gacc = int(cfg.get(\"grad_accum_steps\", 1))\n    use_ind = bool(cfg.get(\"use_indicators\", True))\n\n    datasets, scalers = [], {}\n    feature_cols = None\n\n    for i, tk in enumerate(cfg[\"ticker_list\"]):\n        df_feat = load_or_build_features(tk, cfg, cfg[\"valid_end_date\"])\n        if df_feat is None or df_feat.empty: continue\n        if feature_cols is None: feature_cols = list(df_feat.columns)\n        train_df = df_feat.loc[:pd.to_datetime(cfg[\"train_end_date\"])]\n        if len(train_df) < ctx + 2: continue\n        ts = fit_scalers_per_ticker(train_df, feature_cols)\n        scalers[tk] = {\"x_mean\": ts.x.mean_.tolist(), \"x_scale\": ts.x.scale_.tolist(),\n                       \"y_price_mean\": ts.y_price.mean_.tolist(), \"y_price_scale\": ts.y_price.scale_.tolist(),\n                       \"feature_cols\": feature_cols}\n        X = train_df[feature_cols].values.astype(np.float32)\n        P = train_df[[\"Open\",\"Close\",\"Low\",\"High\"]].values.astype(np.float32)\n        V = train_df[[\"Volume\"]].values.astype(np.float32).ravel()\n        ds = WindowDataset(X, P, V, ctx=ctx, sx=ts.x, sy_price=ts.y_price, ticker_id=i)\n        if len(ds) > 0: datasets.append(ds)\n\n    if not datasets: raise RuntimeError(\"No training data after preprocessing.\")\n\n    train_ds = ConcatDataset(datasets)\n    loader = DataLoader(train_ds, batch_size=int(cfg[\"batch_size\"]), shuffle=True, drop_last=True,\n                        num_workers=2, pin_memory=True, persistent_workers=True)\n\n    f_in = len(feature_cols)\n    model = KronosP100(f_in, cfg).to(dev)\n\n    opt = torch.optim.AdamW(model.parameters(), lr=float(cfg[\"learning_rate\"]), weight_decay=float(cfg.get(\"weight_decay\",0.0)))\n    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1,int(cfg[\"epochs\"])))\n    mse = nn.MSELoss(); ce  = nn.CrossEntropyLoss()\n    lam = float(cfg.get(\"lambda_dir\", 0.25))\n    scaler = GradScaler(enabled=(dev.type==\"cuda\"))\n\n    torch.backends.cudnn.benchmark = True\n\n    def dir_logits(pred_close_std, last_close_std):\n        d = pred_close_std - last_close_std\n        return torch.stack([-d, -torch.abs(d), d], dim=1)\n\n    epochs = int(cfg[\"epochs\"])\n    model.train()\n    for ep in range(1, epochs+1):\n        running = 0.0\n        opt.zero_grad(set_to_none=True)\n        for step, batch in enumerate(tqdm(loader, desc=f\"Epoch {ep}/{epochs}\", leave=False), start=1):\n            xb, y_price_next, y_vol_next = batch\n            xb = xb.to(dev); y_price_next = y_price_next.to(dev); y_vol_next = y_vol_next.to(dev)\n\n            with autocast(enabled=(dev.type==\"cuda\")):\n                price_std, vol_log = model(xb)\n                vol_log = vol_log.squeeze(1)\n                loss_price = mse(price_std, y_price_next)\n                loss_vol   = mse(vol_log, y_vol_next)\n                last_close_std = xb[:, -1, feature_cols.index(\"Close\")]\n                close_pred_std = price_std[:, 1]\n                true_diff = y_price_next[:,1] - last_close_std\n                y_dir = torch.where(true_diff>0, 2, torch.where(true_diff<0, 0, 1))\n                logits = dir_logits(close_pred_std, last_close_std)\n                loss_dir = ce(logits, y_dir.long())\n                loss = loss_price + loss_vol + lam * loss_dir\n\n            loss = loss / gacc\n            scaler.scale(loss).backward()\n\n            if step % gacc == 0:\n                scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True)\n\n            running += float(loss.item()) * xb.size(0) * gacc\n\n        running /= len(train_ds)\n        sched.step()\n        print(f\"[Epoch {ep}] loss={running:.6f} lr={sched.get_last_lr()[0]:.2e}\")\n\n    torch.save({\"state_dict\": model.state_dict(), \"cfg\": cfg, \"feature_cols\": feature_cols}, cfg[\"model_path\"])\n    with open(cfg[\"scaler_path\"], \"wb\") as f: pickle.dump(scalers, f)\n    print(\"OK\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:31:59.525751Z","iopub.execute_input":"2025-09-11T13:31:59.526166Z","iopub.status.idle":"2025-09-11T13:31:59.532400Z","shell.execute_reply.started":"2025-09-11T13:31:59.526132Z","shell.execute_reply":"2025-09-11T13:31:59.531762Z"}},"outputs":[{"name":"stdout","text":"Writing train.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile predict.py\nimport json, pickle, numpy as np, pandas as pd, torch\nfrom util import load_cfg, ensure_dir, set_seed, load_or_build_features, KronosP100, device_from_cfg\n\ndef _consistency(o,c,l,h):\n    lo = float(min(o,c,l,h)); hi = float(max(o,c,l,h))\n    o = float(min(max(o, lo), hi)); c = float(min(max(c, lo), hi))\n    return o, c, lo, hi\n\ndef main():\n    cfg = load_cfg(\"config.yaml\"); set_seed(cfg.get(\"random_seed\",123)); ensure_dir(cfg[\"artifacts_dir\"])\n    dev = device_from_cfg(cfg)\n    ckpt = torch.load(cfg[\"model_path\"], map_location=\"cpu\"); feature_cols = ckpt[\"feature_cols\"]\n    model = KronosP100(len(feature_cols), cfg).to(dev); model.load_state_dict(ckpt[\"state_dict\"]); model.eval()\n    with open(cfg[\"scaler_path\"], \"rb\") as f: scalers = pickle.load(f)\n    ctx = int(cfg[\"context_length\"])\n    v_start = pd.to_datetime(cfg[\"valid_start_date\"]).date(); v_end = pd.to_datetime(cfg[\"valid_end_date\"]).date()\n    results = {}\n    for tk in cfg[\"ticker_list\"]:\n        df_feat = load_or_build_features(tk, cfg, cfg[\"valid_end_date\"])\n        if df_feat is None or df_feat.empty or tk not in scalers: continue\n        if not all(c in df_feat.columns for c in feature_cols): continue\n        sx_mean = np.array(scalers[tk][\"x_mean\"], dtype=np.float32)\n        sx_scale= np.where(np.array(scalers[tk][\"x_scale\"], dtype=np.float32)==0, 1.0, np.array(scalers[tk][\"x_scale\"], dtype=np.float32))\n        py_mean = np.array(scalers[tk][\"y_price_mean\"], dtype=np.float32)\n        py_scale= np.where(np.array(scalers[tk][\"y_price_scale\"], dtype=np.float32)==0, 1.0, np.array(scalers[tk][\"y_price_scale\"], dtype=np.float32))\n        X = df_feat[feature_cols].values.astype(np.float32); dates = df_feat.index\n        pred_map = {}\n        for end_idx in range(ctx-1, len(df_feat)-1):\n            d = dates[end_idx+1].date()\n            if d < v_start or d > v_end: continue\n            x_ctx = X[end_idx-(ctx-1):end_idx+1]\n            x_norm = (x_ctx - sx_mean) / sx_scale\n            with torch.no_grad():\n                price_std, vol_log = model(torch.tensor(x_norm[None, ...], dtype=torch.float32, device=dev))\n                price_std = price_std.cpu().numpy()[0]; vol_log = vol_log.cpu().numpy()[0,0]\n            price = price_std * py_scale + py_mean\n            o, c, l, h = float(price[0]), float(price[1]), float(price[2]), float(price[3])\n            o, c, l, h = _consistency(o,c,l,h)\n            vol = int(np.clip(np.expm1(vol_log), 0, 5e9))\n            pred_map[str(d)] = {\"Open\": float(f\"{o:.2f}\"), \"Close\": float(f\"{c:.2f}\"), \"Low\": float(f\"{l:.2f}\"), \"High\": float(f\"{h:.2f}\"), \"Volume\": vol}\n        if pred_map: results[tk] = pred_map\n    with open(cfg[\"predictions_path\"], \"w\") as f: json.dump(results, f, indent=2)\n    print(\"OK\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:35:42.289726Z","iopub.execute_input":"2025-09-11T13:35:42.290315Z","iopub.status.idle":"2025-09-11T13:35:42.295823Z","shell.execute_reply.started":"2025-09-11T13:35:42.290288Z","shell.execute_reply":"2025-09-11T13:35:42.295267Z"}},"outputs":[{"name":"stdout","text":"Writing predict.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile evaluate.py\nimport json, numpy as np, pandas as pd, yfinance as yf, yaml, os\n\nTARGETS = [\"Open\",\"Close\",\"Low\",\"High\",\"Volume\"]\n\ndef _clean(df):\n    if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)\n    cols = [str(c).strip().title() for c in df.columns]; df = df.copy(); df.columns = cols\n    out = pd.DataFrame(index=df.index)\n    for name in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]:\n        ser = df[name] if name in df.columns else pd.Series(index=df.index, dtype=float)\n        out[name] = ser\n    return out.dropna(subset=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n\ndef _pos(index, d):\n    loc = index.get_loc(d)\n    if isinstance(loc, slice): return int(range(loc.start, loc.stop)[-1])\n    if isinstance(loc, (np.ndarray, list)): return int(loc[-1])\n    return int(loc)\n\ndef _scalar(a, r, c): return float(a[int(r), int(c)])\n\ndef _cls(y_t, y_tp1):\n    if y_tp1 > y_t: return 1\n    if y_tp1 < y_t: return -1\n    return 0\n\ndef _macro(trues, preds):\n    classes = [-1,0,1]\n    trues = np.asarray(trues); preds = np.asarray(preds)\n    acc = float((trues==preds).mean()) if len(trues) else 0.0\n    P=R=F=0.0\n    for c in classes:\n        tp = np.sum((preds==c)&(trues==c)); fp = np.sum((preds==c)&(trues!=c)); fn = np.sum((preds!=c)&(trues==c))\n        p = tp/(tp+fp) if (tp+fp)>0 else 0.0; r = tp/(tp+fn) if (tp+fn)>0 else 0.0; f = (2*p*r)/(p+r) if (p+r)>0 else 0.0\n        P+=p; R+=r; F+=f\n    return float(P/3), float(R/3), float(F/3), acc\n\ndef main():\n    with open(\"config.yaml\",\"r\") as f: cfg = yaml.safe_load(f)\n    os.makedirs(cfg[\"artifacts_dir\"], exist_ok=True)\n    with open(cfg[\"predictions_path\"], \"r\") as f: preds = json.load(f)\n    v_end = pd.to_datetime(cfg[\"valid_end_date\"])\n    rows = []\n    for t in sorted(preds.keys()):\n        gt = yf.download(t, start=cfg[\"valid_start_date\"], end=(v_end + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\"), progress=False, auto_adjust=False, group_by=\"column\")\n        if gt is None or gt.empty: continue\n        gt = _clean(gt); gt.index = pd.to_datetime(gt.index).tz_localize(None); gt = gt.sort_index(); gt = gt[~gt.index.duplicated(keep=\"last\")]\n        p_dates = [pd.to_datetime(d) for d in preds[t].keys()]\n        idx = sorted(set(gt.index) & set(p_dates))\n        if not idx: continue\n        a = gt[[\"Open\",\"Close\",\"Low\",\"High\",\"Volume\"]].to_numpy()\n        m = {\"Open\":0,\"Close\":1,\"Low\":2,\"High\":3,\"Volume\":4}\n        for target in TARGETS:\n            j = m[target]\n            y_true, y_pred, dir_true, dir_pred = [], [], [], []\n            for d in idx:\n                try: pos = _pos(gt.index, d)\n                except KeyError: continue\n                if pos<=0: continue\n                gt_today = _scalar(a, pos, j); prev_val = _scalar(a, pos-1, j)\n                pj = preds[t][str(d.date())].get(target)\n                if pj is None: continue\n                pj = float(pj)\n                y_true.append(gt_today); y_pred.append(pj)\n                dir_true.append(_cls(prev_val, gt_today)); dir_pred.append(_cls(prev_val, pj))\n            if not y_true: continue\n            y_true = np.asarray(y_true, dtype=float); y_pred = np.asarray(y_pred, dtype=float)\n            mse = float(np.mean((y_true - y_pred)**2))\n            P,R,F,Acc = _macro(dir_true, dir_pred) if dir_true and dir_pred else (0,0,0,0)\n            rows.append([t, target, round(mse,6), round(P,6), round(R,6), round(F,6), round(Acc,6)])\n    out = cfg[\"evaluation_csv\"]\n    if rows:\n        df = pd.DataFrame(rows, columns=[\"Ticker\",\"Target\",\"MSE\",\"Precision\",\"Recall\",\"F1\",\"Accuracy\"])\n        order = {k:i for i,k in enumerate([\"Open\",\"Close\",\"Low\",\"High\",\"Volume\"])}\n        df = df.sort_values(by=[\"Ticker\",\"Target\"], key=lambda s: s.map(order) if s.name==\"Target\" else s)\n        df.to_csv(out, index=False); print(\"OK\")\n    else:\n        print(\"OK\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:35:48.444115Z","iopub.execute_input":"2025-09-11T13:35:48.444405Z","iopub.status.idle":"2025-09-11T13:35:48.450637Z","shell.execute_reply.started":"2025-09-11T13:35:48.444384Z","shell.execute_reply":"2025-09-11T13:35:48.449869Z"}},"outputs":[{"name":"stdout","text":"Writing evaluate.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!python train.py\n!python predict.py\n!python evaluate.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:35:56.149476Z","iopub.execute_input":"2025-09-11T13:35:56.149943Z","iopub.status.idle":"2025-09-11T13:44:03.738823Z","shell.execute_reply.started":"2025-09-11T13:35:56.149917Z","shell.execute_reply":"2025-09-11T13:44:03.738051Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n/kaggle/working/train.py:52: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler(enabled=(dev.type==\"cuda\"))\nEpoch 1/18:   0%|                                       | 0/501 [00:00<?, ?it/s]/kaggle/working/train.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(dev.type==\"cuda\")):\n[Epoch 1] loss=30.457914 lr=2.98e-04                                            \n[Epoch 2] loss=3.188442 lr=2.91e-04                                             \n[Epoch 3] loss=2.089082 lr=2.80e-04                                             \n[Epoch 4] loss=1.600803 lr=2.65e-04                                             \n[Epoch 5] loss=1.399884 lr=2.46e-04                                             \n[Epoch 6] loss=1.294443 lr=2.25e-04                                             \n[Epoch 7] loss=1.226314 lr=2.01e-04                                             \n[Epoch 8] loss=1.179458 lr=1.76e-04                                             \n[Epoch 9] loss=1.133175 lr=1.50e-04                                             \n[Epoch 10] loss=1.122406 lr=1.24e-04                                            \n[Epoch 11] loss=1.084258 lr=9.87e-05                                            \n[Epoch 12] loss=1.067295 lr=7.50e-05                                            \n[Epoch 13] loss=1.054355 lr=5.36e-05                                            \n[Epoch 14] loss=1.043249 lr=3.51e-05                                            \n[Epoch 15] loss=1.040014 lr=2.01e-05                                            \n[Epoch 16] loss=1.021614 lr=9.05e-06                                            \n[Epoch 17] loss=1.019515 lr=2.28e-06                                            \n[Epoch 18] loss=1.017452 lr=0.00e+00                                            \nOK\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\nOK\nOK\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"artifacts/evaluation.csv\")\nwide = (df.melt(id_vars=[\"Ticker\",\"Target\"], var_name=\"Metric\", value_name=\"Val\")\n          .pivot_table(index=\"Ticker\", columns=[\"Target\",\"Metric\"], values=\"Val\"))\ntarget_order = [\"Open\",\"Close\",\"Low\",\"High\",\"Volume\"]\nmetric_order = [\"MSE\",\"Precision\",\"Recall\",\"F1\",\"Accuracy\"]\nwide = wide.reindex(columns=pd.MultiIndex.from_product([target_order, metric_order]))\nwide.to_csv(\"artifacts/evaluation_wide.csv\")\ndisplay(wide.head(15))\nprint(\"Saved:\", \"artifacts/evaluation_wide.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:49:15.397340Z","iopub.execute_input":"2025-09-11T13:49:15.397705Z","iopub.status.idle":"2025-09-11T13:49:15.833118Z","shell.execute_reply.started":"2025-09-11T13:49:15.397674Z","shell.execute_reply":"2025-09-11T13:49:15.832353Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"             Open                                              Close  \\\n              MSE Precision    Recall        F1  Accuracy        MSE   \nTicker                                                                 \nAAPL    10.379968  0.424695  0.402778  0.364984  0.561404  10.690281   \nAMD     29.506822  0.420918  0.379730  0.287279  0.456140  36.985578   \nAMTX     0.108797  0.513333  0.413793  0.363396  0.596491   0.058516   \nAMZN    11.264524  0.543860  0.534392  0.538288  0.824561  17.802480   \nBE       5.426228  0.454268  0.436364  0.394647  0.596491   9.749658   \nBN       0.452202  0.528395  0.529363  0.526273  0.789474   1.267621   \nCEG     47.942251  0.571225  0.540529  0.543290  0.824561  76.832996   \nCLNE     0.004723  0.522746  0.538324  0.518239  0.771930   0.012067   \nCOP      2.916874  0.482883  0.469622  0.463841  0.701754   4.237379   \nCSCO     5.384144  0.464286  0.342857  0.206553  0.403509   6.165392   \nCWEN     0.351628  0.534884  0.494253  0.468262  0.701754   0.417584   \nDVN      1.544968  0.511905  0.345679  0.256368  0.543860   3.827632   \nED       1.198519  0.492593  0.440033  0.429825  0.684211   1.146060   \nENB      0.300830  0.517007  0.425287  0.380986  0.614035   0.390012   \nFSLR    47.707610  0.532917  0.531431  0.526273  0.789474  75.891260   \n\n                                                ...       High            \\\n       Precision    Recall        F1  Accuracy  ...        MSE Precision   \nTicker                                          ...                        \nAAPL    0.297619  0.299835  0.289495  0.438596  ...  14.559836  0.392593   \nAMD     0.424901  0.392583  0.336232  0.526316  ...  34.354926  0.421569   \nAMTX    0.388889  0.402778  0.360269  0.543860  ...   0.095130  0.500000   \nAMZN    0.430556  0.386364  0.368126  0.631579  ...  15.503938  0.516900   \nBE      0.328063  0.331481  0.260907  0.403509  ...   8.265858  0.379365   \nBN      0.366097  0.361869  0.344532  0.526316  ...   0.720054  0.471867   \nCEG     0.317730  0.324167  0.295600  0.526316  ...  53.746456  0.467178   \nCLNE    0.301871  0.323048  0.281183  0.508772  ...   0.010028  0.334656   \nCOP     0.315681  0.315681  0.315681  0.473684  ...   3.953227  0.428775   \nCSCO    0.157895  0.333333  0.214286  0.473684  ...   5.837777  0.128655   \nCWEN    0.205882  0.285304  0.213140  0.421053  ...   0.397923  0.282857   \nDVN     0.175439  0.333333  0.229885  0.526316  ...   2.176567  0.175439   \nED      0.329787  0.331281  0.289915  0.491228  ...   1.143997  0.398582   \nENB     0.494048  0.344444  0.238373  0.491228  ...   0.371182  0.157895   \nFSLR    0.369048  0.366832  0.364984  0.561404  ...  73.105946  0.342172   \n\n                                            Volume                      \\\n          Recall        F1  Accuracy           MSE Precision    Recall   \nTicker                                                                   \nAAPL    0.372840  0.339947  0.543860  1.904794e+15  0.439744  0.367406   \nAMD     0.367860  0.281532  0.473684  1.221499e+15  0.152047  0.333333   \nAMTX    0.430108  0.372222  0.578947  1.070454e+12  0.500000  0.406250   \nAMZN    0.465909  0.466596  0.736842  5.807723e+14  0.368881  0.358561   \nBE      0.371693  0.323685  0.491228  4.042766e+13  0.442890  0.411667   \nBN      0.466749  0.465608  0.701754  6.327283e+12  0.169591  0.333333   \nCEG     0.463193  0.463841  0.701754  5.613969e+12  0.494048  0.344444   \nCLNE    0.344444  0.336847  0.508772  2.298719e+12  0.475340  0.401888   \nCOP     0.416049  0.402778  0.614035  6.543251e+13  0.163743  0.333333   \nCSCO    0.333333  0.185654  0.385965  1.844576e+14  0.166667  0.321839   \nCWEN    0.312258  0.239298  0.421053  1.408883e+11  0.350877  0.348933   \nDVN     0.333333  0.229885  0.526316  1.795698e+13  0.346078  0.344167   \nED      0.381075  0.353602  0.596491  1.192270e+12  0.518519  0.370370   \nENB     0.333333  0.214286  0.473684  4.503919e+12  0.395325  0.383415   \nFSLR    0.341975  0.338272  0.508772  8.760326e+12  0.380180  0.376026   \n\n                            \n              F1  Accuracy  \nTicker                      \nAAPL    0.303431  0.543860  \nAMD     0.208835  0.456140  \nAMTX    0.341880  0.561404  \nAMZN    0.341880  0.561404  \nBE      0.403509  0.649123  \nBN      0.224806  0.508772  \nCEG     0.238373  0.491228  \nCLNE    0.359892  0.596491  \nCOP     0.219608  0.491228  \nCSCO    0.219608  0.491228  \nCWEN    0.340849  0.526316  \nDVN     0.320670  0.491228  \nED      0.304762  0.578947  \nENB     0.370563  0.578947  \nFSLR    0.368394  0.561404  \n\n[15 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"5\" halign=\"left\">Open</th>\n      <th colspan=\"5\" halign=\"left\">Close</th>\n      <th>...</th>\n      <th colspan=\"5\" halign=\"left\">High</th>\n      <th colspan=\"5\" halign=\"left\">Volume</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>MSE</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>MSE</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>...</th>\n      <th>MSE</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>MSE</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n    <tr>\n      <th>Ticker</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AAPL</th>\n      <td>10.379968</td>\n      <td>0.424695</td>\n      <td>0.402778</td>\n      <td>0.364984</td>\n      <td>0.561404</td>\n      <td>10.690281</td>\n      <td>0.297619</td>\n      <td>0.299835</td>\n      <td>0.289495</td>\n      <td>0.438596</td>\n      <td>...</td>\n      <td>14.559836</td>\n      <td>0.392593</td>\n      <td>0.372840</td>\n      <td>0.339947</td>\n      <td>0.543860</td>\n      <td>1.904794e+15</td>\n      <td>0.439744</td>\n      <td>0.367406</td>\n      <td>0.303431</td>\n      <td>0.543860</td>\n    </tr>\n    <tr>\n      <th>AMD</th>\n      <td>29.506822</td>\n      <td>0.420918</td>\n      <td>0.379730</td>\n      <td>0.287279</td>\n      <td>0.456140</td>\n      <td>36.985578</td>\n      <td>0.424901</td>\n      <td>0.392583</td>\n      <td>0.336232</td>\n      <td>0.526316</td>\n      <td>...</td>\n      <td>34.354926</td>\n      <td>0.421569</td>\n      <td>0.367860</td>\n      <td>0.281532</td>\n      <td>0.473684</td>\n      <td>1.221499e+15</td>\n      <td>0.152047</td>\n      <td>0.333333</td>\n      <td>0.208835</td>\n      <td>0.456140</td>\n    </tr>\n    <tr>\n      <th>AMTX</th>\n      <td>0.108797</td>\n      <td>0.513333</td>\n      <td>0.413793</td>\n      <td>0.363396</td>\n      <td>0.596491</td>\n      <td>0.058516</td>\n      <td>0.388889</td>\n      <td>0.402778</td>\n      <td>0.360269</td>\n      <td>0.543860</td>\n      <td>...</td>\n      <td>0.095130</td>\n      <td>0.500000</td>\n      <td>0.430108</td>\n      <td>0.372222</td>\n      <td>0.578947</td>\n      <td>1.070454e+12</td>\n      <td>0.500000</td>\n      <td>0.406250</td>\n      <td>0.341880</td>\n      <td>0.561404</td>\n    </tr>\n    <tr>\n      <th>AMZN</th>\n      <td>11.264524</td>\n      <td>0.543860</td>\n      <td>0.534392</td>\n      <td>0.538288</td>\n      <td>0.824561</td>\n      <td>17.802480</td>\n      <td>0.430556</td>\n      <td>0.386364</td>\n      <td>0.368126</td>\n      <td>0.631579</td>\n      <td>...</td>\n      <td>15.503938</td>\n      <td>0.516900</td>\n      <td>0.465909</td>\n      <td>0.466596</td>\n      <td>0.736842</td>\n      <td>5.807723e+14</td>\n      <td>0.368881</td>\n      <td>0.358561</td>\n      <td>0.341880</td>\n      <td>0.561404</td>\n    </tr>\n    <tr>\n      <th>BE</th>\n      <td>5.426228</td>\n      <td>0.454268</td>\n      <td>0.436364</td>\n      <td>0.394647</td>\n      <td>0.596491</td>\n      <td>9.749658</td>\n      <td>0.328063</td>\n      <td>0.331481</td>\n      <td>0.260907</td>\n      <td>0.403509</td>\n      <td>...</td>\n      <td>8.265858</td>\n      <td>0.379365</td>\n      <td>0.371693</td>\n      <td>0.323685</td>\n      <td>0.491228</td>\n      <td>4.042766e+13</td>\n      <td>0.442890</td>\n      <td>0.411667</td>\n      <td>0.403509</td>\n      <td>0.649123</td>\n    </tr>\n    <tr>\n      <th>BN</th>\n      <td>0.452202</td>\n      <td>0.528395</td>\n      <td>0.529363</td>\n      <td>0.526273</td>\n      <td>0.789474</td>\n      <td>1.267621</td>\n      <td>0.366097</td>\n      <td>0.361869</td>\n      <td>0.344532</td>\n      <td>0.526316</td>\n      <td>...</td>\n      <td>0.720054</td>\n      <td>0.471867</td>\n      <td>0.466749</td>\n      <td>0.465608</td>\n      <td>0.701754</td>\n      <td>6.327283e+12</td>\n      <td>0.169591</td>\n      <td>0.333333</td>\n      <td>0.224806</td>\n      <td>0.508772</td>\n    </tr>\n    <tr>\n      <th>CEG</th>\n      <td>47.942251</td>\n      <td>0.571225</td>\n      <td>0.540529</td>\n      <td>0.543290</td>\n      <td>0.824561</td>\n      <td>76.832996</td>\n      <td>0.317730</td>\n      <td>0.324167</td>\n      <td>0.295600</td>\n      <td>0.526316</td>\n      <td>...</td>\n      <td>53.746456</td>\n      <td>0.467178</td>\n      <td>0.463193</td>\n      <td>0.463841</td>\n      <td>0.701754</td>\n      <td>5.613969e+12</td>\n      <td>0.494048</td>\n      <td>0.344444</td>\n      <td>0.238373</td>\n      <td>0.491228</td>\n    </tr>\n    <tr>\n      <th>CLNE</th>\n      <td>0.004723</td>\n      <td>0.522746</td>\n      <td>0.538324</td>\n      <td>0.518239</td>\n      <td>0.771930</td>\n      <td>0.012067</td>\n      <td>0.301871</td>\n      <td>0.323048</td>\n      <td>0.281183</td>\n      <td>0.508772</td>\n      <td>...</td>\n      <td>0.010028</td>\n      <td>0.334656</td>\n      <td>0.344444</td>\n      <td>0.336847</td>\n      <td>0.508772</td>\n      <td>2.298719e+12</td>\n      <td>0.475340</td>\n      <td>0.401888</td>\n      <td>0.359892</td>\n      <td>0.596491</td>\n    </tr>\n    <tr>\n      <th>COP</th>\n      <td>2.916874</td>\n      <td>0.482883</td>\n      <td>0.469622</td>\n      <td>0.463841</td>\n      <td>0.701754</td>\n      <td>4.237379</td>\n      <td>0.315681</td>\n      <td>0.315681</td>\n      <td>0.315681</td>\n      <td>0.473684</td>\n      <td>...</td>\n      <td>3.953227</td>\n      <td>0.428775</td>\n      <td>0.416049</td>\n      <td>0.402778</td>\n      <td>0.614035</td>\n      <td>6.543251e+13</td>\n      <td>0.163743</td>\n      <td>0.333333</td>\n      <td>0.219608</td>\n      <td>0.491228</td>\n    </tr>\n    <tr>\n      <th>CSCO</th>\n      <td>5.384144</td>\n      <td>0.464286</td>\n      <td>0.342857</td>\n      <td>0.206553</td>\n      <td>0.403509</td>\n      <td>6.165392</td>\n      <td>0.157895</td>\n      <td>0.333333</td>\n      <td>0.214286</td>\n      <td>0.473684</td>\n      <td>...</td>\n      <td>5.837777</td>\n      <td>0.128655</td>\n      <td>0.333333</td>\n      <td>0.185654</td>\n      <td>0.385965</td>\n      <td>1.844576e+14</td>\n      <td>0.166667</td>\n      <td>0.321839</td>\n      <td>0.219608</td>\n      <td>0.491228</td>\n    </tr>\n    <tr>\n      <th>CWEN</th>\n      <td>0.351628</td>\n      <td>0.534884</td>\n      <td>0.494253</td>\n      <td>0.468262</td>\n      <td>0.701754</td>\n      <td>0.417584</td>\n      <td>0.205882</td>\n      <td>0.285304</td>\n      <td>0.213140</td>\n      <td>0.421053</td>\n      <td>...</td>\n      <td>0.397923</td>\n      <td>0.282857</td>\n      <td>0.312258</td>\n      <td>0.239298</td>\n      <td>0.421053</td>\n      <td>1.408883e+11</td>\n      <td>0.350877</td>\n      <td>0.348933</td>\n      <td>0.340849</td>\n      <td>0.526316</td>\n    </tr>\n    <tr>\n      <th>DVN</th>\n      <td>1.544968</td>\n      <td>0.511905</td>\n      <td>0.345679</td>\n      <td>0.256368</td>\n      <td>0.543860</td>\n      <td>3.827632</td>\n      <td>0.175439</td>\n      <td>0.333333</td>\n      <td>0.229885</td>\n      <td>0.526316</td>\n      <td>...</td>\n      <td>2.176567</td>\n      <td>0.175439</td>\n      <td>0.333333</td>\n      <td>0.229885</td>\n      <td>0.526316</td>\n      <td>1.795698e+13</td>\n      <td>0.346078</td>\n      <td>0.344167</td>\n      <td>0.320670</td>\n      <td>0.491228</td>\n    </tr>\n    <tr>\n      <th>ED</th>\n      <td>1.198519</td>\n      <td>0.492593</td>\n      <td>0.440033</td>\n      <td>0.429825</td>\n      <td>0.684211</td>\n      <td>1.146060</td>\n      <td>0.329787</td>\n      <td>0.331281</td>\n      <td>0.289915</td>\n      <td>0.491228</td>\n      <td>...</td>\n      <td>1.143997</td>\n      <td>0.398582</td>\n      <td>0.381075</td>\n      <td>0.353602</td>\n      <td>0.596491</td>\n      <td>1.192270e+12</td>\n      <td>0.518519</td>\n      <td>0.370370</td>\n      <td>0.304762</td>\n      <td>0.578947</td>\n    </tr>\n    <tr>\n      <th>ENB</th>\n      <td>0.300830</td>\n      <td>0.517007</td>\n      <td>0.425287</td>\n      <td>0.380986</td>\n      <td>0.614035</td>\n      <td>0.390012</td>\n      <td>0.494048</td>\n      <td>0.344444</td>\n      <td>0.238373</td>\n      <td>0.491228</td>\n      <td>...</td>\n      <td>0.371182</td>\n      <td>0.157895</td>\n      <td>0.333333</td>\n      <td>0.214286</td>\n      <td>0.473684</td>\n      <td>4.503919e+12</td>\n      <td>0.395325</td>\n      <td>0.383415</td>\n      <td>0.370563</td>\n      <td>0.578947</td>\n    </tr>\n    <tr>\n      <th>FSLR</th>\n      <td>47.707610</td>\n      <td>0.532917</td>\n      <td>0.531431</td>\n      <td>0.526273</td>\n      <td>0.789474</td>\n      <td>75.891260</td>\n      <td>0.369048</td>\n      <td>0.366832</td>\n      <td>0.364984</td>\n      <td>0.561404</td>\n      <td>...</td>\n      <td>73.105946</td>\n      <td>0.342172</td>\n      <td>0.341975</td>\n      <td>0.338272</td>\n      <td>0.508772</td>\n      <td>8.760326e+12</td>\n      <td>0.380180</td>\n      <td>0.376026</td>\n      <td>0.368394</td>\n      <td>0.561404</td>\n    </tr>\n  </tbody>\n</table>\n<p>15 rows  25 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"Saved: artifacts/evaluation_wide.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"artifacts/evaluation.csv\")\n\nexcel_path = \"artifacts/evaluation.xlsx\"\ndf.to_excel(excel_path, index=False)\n\nprint(\"Excel file saved at:\", excel_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:51:17.472580Z","iopub.execute_input":"2025-09-11T13:51:17.473254Z","iopub.status.idle":"2025-09-11T13:51:17.844768Z","shell.execute_reply.started":"2025-09-11T13:51:17.473227Z","shell.execute_reply":"2025-09-11T13:51:17.844161Z"}},"outputs":[{"name":"stdout","text":"Excel file saved at: artifacts/evaluation.xlsx\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"summary = (df.groupby(\"Target\")[[\"MSE\",\"Precision\",\"Recall\",\"F1\",\"Accuracy\"]]\n             .mean().round(4).reset_index())\ndisplay(summary)\nsummary.to_csv(\"artifacts/summary_by_target.csv\", index=False)\nprint(\"Saved: artifacts/summary_by_target.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:49:25.081956Z","iopub.execute_input":"2025-09-11T13:49:25.082267Z","iopub.status.idle":"2025-09-11T13:49:25.095789Z","shell.execute_reply.started":"2025-09-11T13:49:25.082245Z","shell.execute_reply":"2025-09-11T13:49:25.095233Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"   Target           MSE  Precision  Recall      F1  Accuracy\n0   Close  5.849120e+01     0.3025  0.3370  0.2820    0.4934\n1    High  5.462950e+01     0.3437  0.3678  0.3074    0.5246\n2     Low  5.024910e+01     0.3648  0.3691  0.3024    0.5171\n3    Open  4.368920e+01     0.4549  0.4239  0.3771    0.6167\n4  Volume  6.033237e+14     0.3618  0.3644  0.2995    0.5404","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Target</th>\n      <th>MSE</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Close</td>\n      <td>5.849120e+01</td>\n      <td>0.3025</td>\n      <td>0.3370</td>\n      <td>0.2820</td>\n      <td>0.4934</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>High</td>\n      <td>5.462950e+01</td>\n      <td>0.3437</td>\n      <td>0.3678</td>\n      <td>0.3074</td>\n      <td>0.5246</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Low</td>\n      <td>5.024910e+01</td>\n      <td>0.3648</td>\n      <td>0.3691</td>\n      <td>0.3024</td>\n      <td>0.5171</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Open</td>\n      <td>4.368920e+01</td>\n      <td>0.4549</td>\n      <td>0.4239</td>\n      <td>0.3771</td>\n      <td>0.6167</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Volume</td>\n      <td>6.033237e+14</td>\n      <td>0.3618</td>\n      <td>0.3644</td>\n      <td>0.2995</td>\n      <td>0.5404</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Saved: artifacts/summary_by_target.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import shutil, os, glob, json, yaml\n\nwith open(\"config.yaml\",\"r\") as f:\n    cfg = yaml.safe_load(f)\nwith open(\"artifacts/config_used.yaml\",\"w\") as f:\n    yaml.safe_dump(cfg, f)\n\ninclude = [\n    \"config.yaml\",\n    \"artifacts/config_used.yaml\",\n    \"artifacts/kronos_p100.pth\",\n    \"artifacts/scalers.pkl\",\n    \"artifacts/predictions.json\",\n    \"artifacts/evaluation.csv\",\n    \"artifacts/evaluation_wide.csv\",\n    \"artifacts/summary_by_target.csv\",\n    \"train.py\", \"predict.py\", \"evaluate.py\", \"util.py\",\n]\ninclude = [p for p in include if os.path.exists(p)]\nshutil.make_archive(\"submission_artifacts\", \"zip\", root_dir=\".\", base_dir=\".\")\n\nos.makedirs(\"submit_bundle\", exist_ok=True)\nfor p in include:\n    tgt = os.path.join(\"submit_bundle\", p)\n    os.makedirs(os.path.dirname(tgt), exist_ok=True)\n    shutil.copy2(p, tgt)\nshutil.make_archive(\"submit_bundle\", \"zip\", \"submit_bundle\")\nprint(\"Created: submit_bundle.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T13:51:56.879421Z","iopub.execute_input":"2025-09-11T13:51:56.879795Z","iopub.status.idle":"2025-09-11T13:51:58.002721Z","shell.execute_reply.started":"2025-09-11T13:51:56.879778Z","shell.execute_reply":"2025-09-11T13:51:58.001928Z"}},"outputs":[{"name":"stdout","text":"Created: submit_bundle.zip\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}